{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import shutil\n",
    "import gensim\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn_vec_zip_path = \"GoogleNews-vectors-negative300.bin.gz\"\n",
    "gn_vec_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "with gzip.open(gn_vec_zip_path, 'rb') as f_in:\n",
    "    with open(gn_vec_path, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = naw.WordEmbsAug(model_type='word2vec', model_path=gn_vec_path, action=\"substitute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>Reason for Exam: CHRONIC LOWER BACK PAIN.  GET...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Narrative &amp; Impression  MR LUMBAR SPINE     Re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>MR LUMBAR SPINE       Reason for Exam: PROGRES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>MR CERVICAL SPINE      Reason for Exam: HAS HX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>MRI lumbar spine     Comparison: No prior     ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   study_id  label                                               text\n",
       "0         2      0  Reason for Exam: CHRONIC LOWER BACK PAIN.  GET...\n",
       "1         3      0  Narrative & Impression  MR LUMBAR SPINE     Re...\n",
       "2         4      0  MR LUMBAR SPINE       Reason for Exam: PROGRES...\n",
       "3         5      0  MR CERVICAL SPINE      Reason for Exam: HAS HX...\n",
       "4         6      0  MRI lumbar spine     Comparison: No prior     ..."
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"KECSACQIProject_DATA_2022-03-31_full dataset for CS.csv\"\n",
    "data = pd.read_csv(data_path, header=0, names=[\"study_id\", \"label\", \"text\"], encoding='unicode_escape')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_text(df,samples=300,pr=0.2):\n",
    "    aug.aug_p=pr\n",
    "    new_text=[]\n",
    "\n",
    "    df_n=df[df.label==1].reset_index(drop=True)\n",
    "    ## data augmentation loop\n",
    "    for i in tqdm(np.random.randint(0,len(df_n),samples)):\n",
    "        text = df_n.iloc[i]['text']\n",
    "        augmented_text = aug.augment(text)\n",
    "        new_text.append(augmented_text)\n",
    "    \n",
    "    ## dataframe\n",
    "    new_data=shuffle(pd.DataFrame({'text':new_text,'label':1}), random_state=random_state)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(X_train):\n",
    "    print(X_train)\n",
    "    vectorizer = CountVectorizer(lowercase=True, max_df=0.9, min_df=2)\n",
    "    X = vectorizer.fit_transform(X_train)\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    X_train = X_train.apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "    texts = []\n",
    "    for text in X_train:\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in vocab:\n",
    "                new_text.append(word)\n",
    "        texts.append(' '.join(new_text))\n",
    "    return pd.Series(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:03<00:00,  1.62s/it]\n",
      "/var/folders/_y/nvgj0qhs1938vytcxtcjgblc0000gn/T/ipykernel_32753/2087078228.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_Y_augmented_train =shuffle(X_Y_train.append(X_Y_augmented).reset_index(drop=True), random_state=random_state)\n",
      "100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "/var/folders/_y/nvgj0qhs1938vytcxtcjgblc0000gn/T/ipykernel_32753/2087078228.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_Y_augmented_train =shuffle(X_Y_train.append(X_Y_augmented).reset_index(drop=True), random_state=random_state)\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.87s/it]\n",
      "/var/folders/_y/nvgj0qhs1938vytcxtcjgblc0000gn/T/ipykernel_32753/2087078228.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_Y_augmented_train =shuffle(X_Y_train.append(X_Y_augmented).reset_index(drop=True), random_state=random_state)\n",
      "100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n",
      "/var/folders/_y/nvgj0qhs1938vytcxtcjgblc0000gn/T/ipykernel_32753/2087078228.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_Y_augmented_train =shuffle(X_Y_train.append(X_Y_augmented).reset_index(drop=True), random_state=random_state)\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.37s/it]\n",
      "/var/folders/_y/nvgj0qhs1938vytcxtcjgblc0000gn/T/ipykernel_32753/2087078228.py:15: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  X_Y_augmented_train =shuffle(X_Y_train.append(X_Y_augmented).reset_index(drop=True), random_state=random_state)\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "folds = KFold(n_splits=k, random_state=random_state, shuffle=True)\n",
    "fold = 0\n",
    "for train_index, test_index in folds.split(data['text'], data['label']):\n",
    "    fold += 1\n",
    "    X_train, Y_train, study_id_train = data['text'][train_index], data['label'][train_index], data['study_id'][train_index]\n",
    "    X_test, Y_test, study_id_test = data['text'][test_index], data['label'][test_index], data['study_id'][test_index]\n",
    "    \n",
    "    # X_train = remove_stop_words(X_train)\n",
    "    # X_test = remove_stop_words(X_test)\n",
    "    X_Y_train = pd.DataFrame({'study_id':study_id_train, 'text':X_train,'label':Y_train})\n",
    "    X_Y_test = pd.DataFrame({'study_id':study_id_test, 'text':X_test,'label':Y_test})\n",
    "    \n",
    "    X_Y_augmented = augment_text(X_Y_train, samples=2)\n",
    "    X_Y_augmented_train =shuffle(X_Y_train.append(X_Y_augmented).reset_index(drop=True), random_state=random_state)\n",
    "\n",
    "    X_Y_train.to_csv(f\"fold_{fold}_train.csv\", index=False)\n",
    "    X_Y_augmented_train.to_csv(f\"fold_{fold}_augmented_train.csv\", index=False)\n",
    "    X_Y_test.to_csv(f\"fold_{fold}_test.csv\", index=False)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
